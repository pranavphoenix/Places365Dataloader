{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEDz-MaPPYSg",
        "outputId": "fdcafffa-d6b5-4ea0-f676-fca0f99084cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_CudaDeviceProperties(name='Tesla V100-SXM2-16GB', major=7, minor=0, total_memory=16160MB, multi_processor_count=80)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(torch.cuda.get_device_properties(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bng5unR8kT1b",
        "outputId": "d3f3c97b-f056-4ce8-b597-6087dee32f96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAI0xLGLPYSp"
      },
      "outputs": [],
      "source": [
        "import sys, os, time, pickle\n",
        "import numpy as np\n",
        "import math\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torch.optim as optim\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "!pip install einops\n",
        "from math import ceil\n",
        "import pywt\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "from torch import nn, einsum\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "# helpers\n",
        "from einops import reduce\n",
        "# !pip install git+https://github.com/ildoonet/pytorch-randaugment\n",
        "# from RandAugment import RandAugment\n",
        "# transform_train = RandAugment(1, 1)\n",
        "\n",
        "# batch_size = 256\n",
        "# transform = transforms.Compose(\n",
        "#         [transforms.Resize([256, ]),\n",
        "#             transforms.ToTensor(),\n",
        "#      transforms.Normalize((0.4915, 0.4822, 0.4466), (0.2470, 0.2435, 0.2616))])\n",
        "\n",
        "!wget http://data.csail.mit.edu/places/places365/places365standard_easyformat.tar\n",
        "!tar -xvf \"/content/places365standard_easyformat.tar\" -C \"/content/\"     #[run this cell to extract tar files]\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y9mzoOP6z1Ps"
      },
      "outputs": [],
      "source": [
        "transform_test = transforms.Compose(\n",
        "        [transforms.Resize([128, ]),\n",
        "         transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4576, 0.4411, 0.4080), (0.2689, 0.2669, 0.2849))])\n",
        "\n",
        "transform_train = transforms.Compose(\n",
        "        [transforms.Resize([128, ]),\n",
        "         transforms.ToTensor(),\n",
        "     transforms.Normalize((0.4577, 0.4413, 0.4078), (0.2695, 0.2671, 0.2853))])\n",
        "\n",
        "trainset = torchvision.datasets.ImageFolder(root='/content/places365_standard/train', transform=transform_train)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.ImageFolder(root='/content/places365_standard/val', transform=transform_test)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JGh0uaGSPYSr"
      },
      "outputs": [],
      "source": [
        "\n",
        "def accuracy(output, target, topk=(1,5)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\n",
        "    prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
        "    \"\"\"\n",
        "    maxk = max(topk)\n",
        "         # sizefunction: the number of total elements\n",
        "    batch_size = target.size(0) \n",
        " \n",
        "         # topk function selects the number of k before output\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "         ##########Do not understand t()k\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))   \n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qrIKQkHwPYSr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.autograd import Function\n",
        "import pywt\n",
        "import torch.nn as nn\n",
        "import functools\n",
        "\n",
        "def sfb1d(lo, hi, g0, g1, mode='zero', dim=-1):\n",
        "    \"\"\" 1D synthesis filter bank of an image tensor\n",
        "    \"\"\"\n",
        "    C = lo.shape[1]\n",
        "    d = dim % 4\n",
        "    # If g0, g1 are not tensors, make them. If they are, then assume that they\n",
        "    # are in the right order\n",
        "    if not isinstance(g0, torch.Tensor):\n",
        "        g0 = torch.tensor(np.copy(np.array(g0).ravel()),\n",
        "                          dtype=torch.float, device=lo.device)\n",
        "    if not isinstance(g1, torch.Tensor):\n",
        "        g1 = torch.tensor(np.copy(np.array(g1).ravel()),\n",
        "                          dtype=torch.float, device=lo.device)\n",
        "    L = g0.numel()\n",
        "    shape = [1,1,1,1]\n",
        "    shape[d] = L\n",
        "    N = 2*lo.shape[d]\n",
        "    # If g aren't in the right shape, make them so\n",
        "    if g0.shape != tuple(shape):\n",
        "        g0 = g0.reshape(*shape)\n",
        "    if g1.shape != tuple(shape):\n",
        "        g1 = g1.reshape(*shape)\n",
        "\n",
        "    s = (2, 1) if d == 2 else (1,2)\n",
        "    g0 = torch.cat([g0]*C,dim=0)\n",
        "    g1 = torch.cat([g1]*C,dim=0)\n",
        "    if mode == 'per' or mode == 'periodization':\n",
        "        y = F.conv_transpose2d(lo, g0, stride=s, groups=C) + \\\n",
        "            F.conv_transpose2d(hi, g1, stride=s, groups=C)\n",
        "        if d == 2:\n",
        "            y[:,:,:L-2] = y[:,:,:L-2] + y[:,:,N:N+L-2]\n",
        "            y = y[:,:,:N]\n",
        "        else:\n",
        "            y[:,:,:,:L-2] = y[:,:,:,:L-2] + y[:,:,:,N:N+L-2]\n",
        "            y = y[:,:,:,:N]\n",
        "        y = roll(y, 1-L//2, dim=dim)\n",
        "    else:\n",
        "        if mode == 'zero' or mode == 'symmetric' or mode == 'reflect' or \\\n",
        "                mode == 'periodic':\n",
        "            pad = (L-2, 0) if d == 2 else (0, L-2)\n",
        "            y = F.conv_transpose2d(lo, g0, stride=s, padding=pad, groups=C) + \\\n",
        "                F.conv_transpose2d(hi, g1, stride=s, padding=pad, groups=C)\n",
        "        else:\n",
        "            raise ValueError(\"Unkown pad type: {}\".format(mode))\n",
        "\n",
        "    return y\n",
        "\n",
        "def reflect(x, minx, maxx):\n",
        "    \"\"\"Reflect the values in matrix *x* about the scalar values *minx* and\n",
        "    *maxx*.  Hence a vector *x* containing a long linearly increasing series is\n",
        "    converted into a waveform which ramps linearly up and down between *minx*\n",
        "    and *maxx*.  If *x* contains integers and *minx* and *maxx* are (integers +\n",
        "    0.5), the ramps will have repeated max and min samples.\n",
        "    .. codeauthor:: Rich Wareham <rjw57@cantab.net>, Aug 2013\n",
        "    .. codeauthor:: Nick Kingsbury, Cambridge University, January 1999.\n",
        "    \"\"\"\n",
        "    x = np.asanyarray(x)\n",
        "    rng = maxx - minx\n",
        "    rng_by_2 = 2 * rng\n",
        "    mod = np.fmod(x - minx, rng_by_2)\n",
        "    normed_mod = np.where(mod < 0, mod + rng_by_2, mod)\n",
        "    out = np.where(normed_mod >= rng, rng_by_2 - normed_mod, normed_mod) + minx\n",
        "    return np.array(out, dtype=x.dtype)\n",
        "\n",
        "def mode_to_int(mode):\n",
        "    if mode == 'zero':\n",
        "        return 0\n",
        "    elif mode == 'symmetric':\n",
        "        return 1\n",
        "    elif mode == 'per' or mode == 'periodization':\n",
        "        return 2\n",
        "    elif mode == 'constant':\n",
        "        return 3\n",
        "    elif mode == 'reflect':\n",
        "        return 4\n",
        "    elif mode == 'replicate':\n",
        "        return 5\n",
        "    elif mode == 'periodic':\n",
        "        return 6\n",
        "    else:\n",
        "        raise ValueError(\"Unkown pad type: {}\".format(mode))\n",
        "\n",
        "def int_to_mode(mode):\n",
        "    if mode == 0:\n",
        "        return 'zero'\n",
        "    elif mode == 1:\n",
        "        return 'symmetric'\n",
        "    elif mode == 2:\n",
        "        return 'periodization'\n",
        "    elif mode == 3:\n",
        "        return 'constant'\n",
        "    elif mode == 4:\n",
        "        return 'reflect'\n",
        "    elif mode == 5:\n",
        "        return 'replicate'\n",
        "    elif mode == 6:\n",
        "        return 'periodic'\n",
        "    else:\n",
        "        raise ValueError(\"Unkown pad type: {}\".format(mode))\n",
        "\n",
        "def afb1d(x, h0, h1, mode='zero', dim=-1):\n",
        "    \"\"\" 1D analysis filter bank (along one dimension only) of an image\n",
        "    Inputs:\n",
        "        x (tensor): 4D input with the last two dimensions the spatial input\n",
        "        h0 (tensor): 4D input for the lowpass filter. Should have shape (1, 1,\n",
        "            h, 1) or (1, 1, 1, w)\n",
        "        h1 (tensor): 4D input for the highpass filter. Should have shape (1, 1,\n",
        "            h, 1) or (1, 1, 1, w)\n",
        "        mode (str): padding method\n",
        "        dim (int) - dimension of filtering. d=2 is for a vertical filter (called\n",
        "            column filtering but filters across the rows). d=3 is for a\n",
        "            horizontal filter, (called row filtering but filters across the\n",
        "            columns).\n",
        "    Returns:\n",
        "        lohi: lowpass and highpass subbands concatenated along the channel\n",
        "            dimension\n",
        "    \"\"\"\n",
        "    C = x.shape[1]\n",
        "    # Convert the dim to positive\n",
        "    d = dim % 4\n",
        "    s = (2, 1) if d == 2 else (1, 2)\n",
        "    N = x.shape[d]\n",
        "    # If h0, h1 are not tensors, make them. If they are, then assume that they\n",
        "    # are in the right order\n",
        "    if not isinstance(h0, torch.Tensor):\n",
        "        h0 = torch.tensor(np.copy(np.array(h0).ravel()[::-1]),\n",
        "                          dtype=torch.float, device=x.device)\n",
        "    if not isinstance(h1, torch.Tensor):\n",
        "        h1 = torch.tensor(np.copy(np.array(h1).ravel()[::-1]),\n",
        "                          dtype=torch.float, device=x.device)\n",
        "    L = h0.numel()\n",
        "    L2 = L // 2\n",
        "    shape = [1,1,1,1]\n",
        "    shape[d] = L\n",
        "    # If h aren't in the right shape, make them so\n",
        "    if h0.shape != tuple(shape):\n",
        "        h0 = h0.reshape(*shape)\n",
        "    if h1.shape != tuple(shape):\n",
        "        h1 = h1.reshape(*shape)\n",
        "    h = torch.cat([h0, h1] * C, dim=0)\n",
        "\n",
        "    if mode == 'per' or mode == 'periodization':\n",
        "        if x.shape[dim] % 2 == 1:\n",
        "            if d == 2:\n",
        "                x = torch.cat((x, x[:,:,-1:]), dim=2)\n",
        "            else:\n",
        "                x = torch.cat((x, x[:,:,:,-1:]), dim=3)\n",
        "            N += 1\n",
        "        x = roll(x, -L2, dim=d)\n",
        "        pad = (L-1, 0) if d == 2 else (0, L-1)\n",
        "        lohi = F.conv2d(x, h, padding=pad, stride=s, groups=C)\n",
        "        N2 = N//2\n",
        "        if d == 2:\n",
        "            lohi[:,:,:L2] = lohi[:,:,:L2] + lohi[:,:,N2:N2+L2]\n",
        "            lohi = lohi[:,:,:N2]\n",
        "        else:\n",
        "            lohi[:,:,:,:L2] = lohi[:,:,:,:L2] + lohi[:,:,:,N2:N2+L2]\n",
        "            lohi = lohi[:,:,:,:N2]\n",
        "    else:\n",
        "        # Calculate the pad size\n",
        "        outsize = pywt.dwt_coeff_len(N, L, mode=mode)\n",
        "        p = 2 * (outsize - 1) - N + L\n",
        "        if mode == 'zero':\n",
        "            # Sadly, pytorch only allows for same padding before and after, if\n",
        "            # we need to do more padding after for odd length signals, have to\n",
        "            # prepad\n",
        "            if p % 2 == 1:\n",
        "                pad = (0, 0, 0, 1) if d == 2 else (0, 1, 0, 0)\n",
        "                x = F.pad(x, pad)\n",
        "            pad = (p//2, 0) if d == 2 else (0, p//2)\n",
        "            # Calculate the high and lowpass\n",
        "            lohi = F.conv2d(x, h, padding=pad, stride=s, groups=C)\n",
        "        elif mode == 'symmetric' or mode == 'reflect' or mode == 'periodic':\n",
        "            pad = (0, 0, p//2, (p+1)//2) if d == 2 else (p//2, (p+1)//2, 0, 0)\n",
        "            x = mypad(x, pad=pad, mode=mode)\n",
        "            lohi = F.conv2d(x, h, stride=s, groups=C)\n",
        "        else:\n",
        "            raise ValueError(\"Unkown pad type: {}\".format(mode))\n",
        "\n",
        "    return lohi\n",
        "\n",
        "\n",
        "\n",
        "class AFB2D(Function):\n",
        "    \"\"\" Does a single level 2d wavelet decomposition of an input. Does separate\n",
        "    row and column filtering by two calls to\n",
        "    :py:func:`pytorch_wavelets.dwt.lowlevel.afb1d`\n",
        "    Needs to have the tensors in the right form. Because this function defines\n",
        "    its own backward pass, saves on memory by not having to save the input\n",
        "    tensors.\n",
        "    Inputs:\n",
        "        x (torch.Tensor): Input to decompose\n",
        "        h0_row: row lowpass\n",
        "        h1_row: row highpass\n",
        "        h0_col: col lowpass\n",
        "        h1_col: col highpass\n",
        "        mode (int): use mode_to_int to get the int code here\n",
        "    We encode the mode as an integer rather than a string as gradcheck causes an\n",
        "    error when a string is provided.\n",
        "    Returns:\n",
        "        y: Tensor of shape (N, C*4, H, W)\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, h0_row, h1_row, h0_col, h1_col, mode):\n",
        "        ctx.save_for_backward(h0_row, h1_row, h0_col, h1_col)\n",
        "        ctx.shape = x.shape[-2:]\n",
        "        mode = int_to_mode(mode)\n",
        "        ctx.mode = mode\n",
        "        lohi = afb1d(x, h0_row, h1_row, mode=mode, dim=3)\n",
        "        y = afb1d(lohi, h0_col, h1_col, mode=mode, dim=2)\n",
        "        s = y.shape\n",
        "        y = y.reshape(s[0], -1, 4, s[-2], s[-1])\n",
        "        low = y[:,:,0].contiguous()\n",
        "        highs = y[:,:,1:].contiguous()\n",
        "        return low, highs\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, low, highs):\n",
        "        dx = None\n",
        "        if ctx.needs_input_grad[0]:\n",
        "            mode = ctx.mode\n",
        "            h0_row, h1_row, h0_col, h1_col = ctx.saved_tensors\n",
        "            lh, hl, hh = torch.unbind(highs, dim=2)\n",
        "            lo = sfb1d(low, lh, h0_col, h1_col, mode=mode, dim=2)\n",
        "            hi = sfb1d(hl, hh, h0_col, h1_col, mode=mode, dim=2)\n",
        "            dx = sfb1d(lo, hi, h0_row, h1_row, mode=mode, dim=3)\n",
        "            if dx.shape[-2] > ctx.shape[-2] and dx.shape[-1] > ctx.shape[-1]:\n",
        "                dx = dx[:,:,:ctx.shape[-2], :ctx.shape[-1]]\n",
        "            elif dx.shape[-2] > ctx.shape[-2]:\n",
        "                dx = dx[:,:,:ctx.shape[-2]]\n",
        "            elif dx.shape[-1] > ctx.shape[-1]:\n",
        "                dx = dx[:,:,:,:ctx.shape[-1]]\n",
        "        return dx, None, None, None, None, None\n",
        "\n",
        "\n",
        "def prep_filt_afb2d(h0_col, h1_col, h0_row=None, h1_row=None, device=device):\n",
        "    \"\"\"\n",
        "    Prepares the filters to be of the right form for the afb2d function.  In\n",
        "    particular, makes the tensors the right shape. It takes mirror images of\n",
        "    them as as afb2d uses conv2d which acts like normal correlation.\n",
        "    Inputs:\n",
        "        h0_col (array-like): low pass column filter bank\n",
        "        h1_col (array-like): high pass column filter bank\n",
        "        h0_row (array-like): low pass row filter bank. If none, will assume the\n",
        "            same as column filter\n",
        "        h1_row (array-like): high pass row filter bank. If none, will assume the\n",
        "            same as column filter\n",
        "        device: which device to put the tensors on to\n",
        "    Returns:\n",
        "        (h0_col, h1_col, h0_row, h1_row)\n",
        "    \"\"\"\n",
        "    h0_col, h1_col = prep_filt_afb1d(h0_col, h1_col, device)\n",
        "    if h0_row is None:\n",
        "        h0_row, h1_col = h0_col, h1_col\n",
        "    else:\n",
        "        h0_row, h1_row = prep_filt_afb1d(h0_row, h1_row, device)\n",
        "\n",
        "    h0_col = h0_col.reshape((1, 1, -1, 1))\n",
        "    h1_col = h1_col.reshape((1, 1, -1, 1))\n",
        "    h0_row = h0_row.reshape((1, 1, 1, -1))\n",
        "    h1_row = h1_row.reshape((1, 1, 1, -1))\n",
        "    return h0_col, h1_col, h0_row, h1_row\n",
        "\n",
        "\n",
        "def prep_filt_afb1d(h0, h1, device=device):\n",
        "    \"\"\"\n",
        "    Prepares the filters to be of the right form for the afb2d function.  In\n",
        "    particular, makes the tensors the right shape. It takes mirror images of\n",
        "    them as as afb2d uses conv2d which acts like normal correlation.\n",
        "    Inputs:\n",
        "        h0 (array-like): low pass column filter bank\n",
        "        h1 (array-like): high pass column filter bank\n",
        "        device: which device to put the tensors on to\n",
        "    Returns:\n",
        "        (h0, h1)\n",
        "    \"\"\"\n",
        "    h0 = np.array(h0[::-1]).ravel()\n",
        "    h1 = np.array(h1[::-1]).ravel()\n",
        "    t = torch.get_default_dtype()\n",
        "    h0 = torch.tensor(h0, device=device, dtype=t).reshape((1, 1, -1))\n",
        "    h1 = torch.tensor(h1, device=device, dtype=t).reshape((1, 1, -1))\n",
        "    return h0, h1\n",
        "\n",
        "class DWTForward(nn.Module):\n",
        "    \"\"\" Performs a 2d DWT Forward decomposition of an image\n",
        "    Args:\n",
        "        J (int): Number of levels of decomposition\n",
        "        wave (str or pywt.Wavelet or tuple(ndarray)): Which wavelet to use.\n",
        "            Can be:\n",
        "            1) a string to pass to pywt.Wavelet constructor\n",
        "            2) a pywt.Wavelet class\n",
        "            3) a tuple of numpy arrays, either (h0, h1) or (h0_col, h1_col, h0_row, h1_row)\n",
        "        mode (str): 'zero', 'symmetric', 'reflect' or 'periodization'. The\n",
        "            padding scheme\n",
        "        \"\"\"\n",
        "    def __init__(self, J=1, wave='db1', mode='zero'):\n",
        "        super().__init__()\n",
        "        if isinstance(wave, str):\n",
        "            wave = pywt.Wavelet(wave)\n",
        "        if isinstance(wave, pywt.Wavelet):\n",
        "            h0_col, h1_col = wave.dec_lo, wave.dec_hi\n",
        "            h0_row, h1_row = h0_col, h1_col\n",
        "        else:\n",
        "            if len(wave) == 2:\n",
        "                h0_col, h1_col = wave[0], wave[1]\n",
        "                h0_row, h1_row = h0_col, h1_col\n",
        "            elif len(wave) == 4:\n",
        "                h0_col, h1_col = wave[0], wave[1]\n",
        "                h0_row, h1_row = wave[2], wave[3]\n",
        "\n",
        "        # Prepare the filters\n",
        "        filts = prep_filt_afb2d(h0_col, h1_col, h0_row, h1_row)\n",
        "        self.register_buffer('h0_col', filts[0])\n",
        "        self.register_buffer('h1_col', filts[1])\n",
        "        self.register_buffer('h0_row', filts[2])\n",
        "        self.register_buffer('h1_row', filts[3])\n",
        "        self.J = J\n",
        "        self.mode = mode\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Forward pass of the DWT.\n",
        "        Args:\n",
        "            x (tensor): Input of shape :math:`(N, C_{in}, H_{in}, W_{in})`\n",
        "        Returns:\n",
        "            (yl, yh)\n",
        "                tuple of lowpass (yl) and bandpass (yh) coefficients.\n",
        "                yh is a list of length J with the first entry\n",
        "                being the finest scale coefficients. yl has shape\n",
        "                :math:`(N, C_{in}, H_{in}', W_{in}')` and yh has shape\n",
        "                :math:`list(N, C_{in}, 3, H_{in}'', W_{in}'')`. The new\n",
        "                dimension in yh iterates over the LH, HL and HH coefficients.\n",
        "        Note:\n",
        "            :math:`H_{in}', W_{in}', H_{in}'', W_{in}''` denote the correctly\n",
        "            downsampled shapes of the DWT pyramid.\n",
        "        \"\"\"\n",
        "        yh = []\n",
        "        ll = x\n",
        "        mode = mode_to_int(self.mode)\n",
        "\n",
        "        # Do a multilevel transform\n",
        "        for j in range(self.J):\n",
        "            # Do 1 level of the transform\n",
        "            ll, high = AFB2D.apply(\n",
        "                ll, self.h0_col, self.h1_col, self.h0_row, self.h1_row, mode)\n",
        "            yh.append(high)\n",
        "\n",
        "        return ll, yh\n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UkeZVJ6YPYSv"
      },
      "outputs": [],
      "source": [
        "from numpy.lib.function_base import hamming\n",
        "import pywt\n",
        "class Waveblock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        # mult_dim = 32,\n",
        "        mult = 2,\n",
        "        ff_channel = 16,\n",
        "        final_dim = 16,\n",
        "        dropout = 0.5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        \n",
        "        # self.feedforward = nn.Sequential(\n",
        "        #         nn.Conv2d(final_dim * 4, mult_dim,1),\n",
        "        #         nn.GELU(),\n",
        "        #         nn.Dropout(dropout),\n",
        "        #         nn.Conv2d(mult_dim, ff_channel, 1),\n",
        "        #         nn.Dropout(dropout)\n",
        "        #     )\n",
        "        self.feedforward = nn.Sequential(\n",
        "                nn.Conv2d(final_dim, final_dim*mult,1),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Conv2d(final_dim*mult, ff_channel, 1),\n",
        "#                 nn.Dropout(dropout),\n",
        "            )\n",
        "      \n",
        "        self.ff1 = nn.ConvTranspose2d(ff_channel, final_dim, 4, stride=2, padding=1)\n",
        "#         self.ff2 = nn.ConvTranspose2d(ff_channel, int(final_dim/2), 6, stride=4, padding=1)\n",
        "#         self.ff3 = nn.ConvTranspose2d(ff_channel, int(final_dim/4), 10, stride=8, padding=1)\n",
        "#         self.ff4 = nn.ConvTranspose2d(ff_channel, int(final_dim/4), 18, stride=16, padding=1)\n",
        "        # self.ff5 = nn.ConvTranspose2d(ff_channel, int(final_dim/5), 34, stride=32, padding=1)\n",
        "\n",
        "\n",
        "        self.depthconv = nn.Sequential(\n",
        "            \n",
        "#             nn.Conv2d(final_dim, final_dim, 5, groups=final_dim, padding=\"same\"),\n",
        "#             nn.GELU(),\n",
        "            nn.BatchNorm2d(final_dim),\n",
        "        )\n",
        "\n",
        "        self.reduction = nn.Conv2d(final_dim, int(final_dim/4), 1)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        \n",
        "        x = self.reduction(x)\n",
        "     \n",
        "\n",
        "        xf1 = DWTForward(J=1, wave='db1').cuda()\n",
        "#         xf2 = DWTForward(J=2, mode='zero', wave='db1').cuda()\n",
        "#         xf3 = DWTForward(J=3, mode='zero', wave='db1').cuda()\n",
        "#         xf4 = DWTForward(J=4, mode='zero', wave='db1').cuda()\n",
        "        # xf5 = DWTForward(J=5, mode='zero', wave='db1').cuda()\n",
        "        \n",
        "        Y1, Yh = xf1(x)\n",
        "#         Y2, Yh = xf2(x)\n",
        "#         Y3, Yh = xf3(x)\n",
        "#         Y4, Yh = xf4(x)\n",
        "        # Y5, Yh = xf5(x)\n",
        "\n",
        "        # x1 = torch.reshape(Yh[0], (b, c*3, int(h/2), int(h/2)))\n",
        "        # x2 = torch.reshape(Yh[1], (b, c*3, int(h/4), int(w/4)))\n",
        "        # x3 = torch.reshape(Yh[2], (b, c*3, int(h/8), int(w/8)))\n",
        "        # x4 = torch.reshape(Yh[3], (b, c*3, int(h/16), int(w/16)))\n",
        "\n",
        "        x1 = torch.reshape(Yh[0], (b, int(c*3/4), int(h/2), int(h/2)))\n",
        "#         x2 = torch.reshape(Yh[1], (b, int(c*3/4), int(h/4), int(w/4)))\n",
        "#         x3 = torch.reshape(Yh[2], (b, int(c*3/4), int(h/8), int(w/8)))\n",
        "#         x4 = torch.reshape(Yh[3], (b, int(c*3/4), int(h/16), int(w/16)))\n",
        "        # x5 = torch.reshape(Yh[4], (b, int(c*3/4), int(h/32), int(w/32)))\n",
        "\n",
        "\n",
        "        x1 = torch.cat((Y1,x1), dim = 1)\n",
        "#         x2 = torch.cat((Y2,x2), dim = 1)\n",
        "#         x3 = torch.cat((Y3,x3), dim = 1)\n",
        "#         x4 = torch.cat((Y4,x4), dim = 1)\n",
        "        # x5 = torch.cat((Y5,x5), dim = 1)\n",
        "\n",
        "        x1 = self.feedforward(x1)\n",
        "#         x2 = self.feedforward(x2)\n",
        "#         x3 = self.feedforward(x3)\n",
        "#         x4 = self.feedforward(x4)\n",
        "        # x5 = self.feedforward(x5)\n",
        "\n",
        "        x1 = self.ff1(x1)\n",
        "#         x2 = self.ff2(x2)\n",
        "#         x3 = self.ff3(x3)\n",
        "#         x4 = self.ff4(x4)\n",
        "        # x5 = self.ff5(x5)\n",
        "        \n",
        "#         x = torch.cat((x1,x2,x3,x4), dim = 1)\n",
        "        # x = torch.cat((x1,x2,x3), dim = 1)\n",
        "#         x = torch.cat((x1,x2), dim = 1)\n",
        "        x = self.depthconv(x1)\n",
        "        \n",
        "        return x\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kdtNR9KHPYSx"
      },
      "outputs": [],
      "source": [
        "class WaveMix(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        num_classes,\n",
        "        depth,\n",
        "        # mult_dim = 32,\n",
        "        mult = 2,\n",
        "        ff_channel = 16,\n",
        "        final_dim = 16,\n",
        "        dropout = 0.,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            # self.layers.append(Waveblock(mult_dim = mult_dim, ff_channel = ff_channel, final_dim = final_dim, dropout = dropout))\n",
        "            self.layers.append(Waveblock(mult = mult, ff_channel = ff_channel, final_dim = final_dim, dropout = dropout))\n",
        "        \n",
        "        self.pool = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            Rearrange('... () () -> ...'),\n",
        "            nn.Linear(final_dim, num_classes)\n",
        "        )\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, int(final_dim/2), 3, 1, 1),\n",
        "            nn.Conv2d(int(final_dim/2), final_dim, 3, 1, 1)\n",
        "        )\n",
        "\n",
        "      \n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.conv(img)   \n",
        "            \n",
        "        for attn in self.layers:\n",
        "            x = attn(x) + x\n",
        "\n",
        "        out = self.pool(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqygS8HjPYSy",
        "outputId": "94171235-b77a-481c-bbe2-454ea4cabb97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1        [-1, 128, 128, 128]           3,584\n",
            "            Conv2d-2        [-1, 256, 128, 128]         295,168\n",
            "            Conv2d-3         [-1, 64, 128, 128]          16,448\n",
            "            Conv2d-4          [-1, 512, 64, 64]         131,584\n",
            "              GELU-5          [-1, 512, 64, 64]               0\n",
            "           Dropout-6          [-1, 512, 64, 64]               0\n",
            "            Conv2d-7          [-1, 256, 64, 64]         131,328\n",
            "   ConvTranspose2d-8        [-1, 256, 128, 128]       1,048,832\n",
            "       BatchNorm2d-9        [-1, 256, 128, 128]             512\n",
            "        Waveblock-10        [-1, 256, 128, 128]               0\n",
            "           Conv2d-11         [-1, 64, 128, 128]          16,448\n",
            "           Conv2d-12          [-1, 512, 64, 64]         131,584\n",
            "             GELU-13          [-1, 512, 64, 64]               0\n",
            "          Dropout-14          [-1, 512, 64, 64]               0\n",
            "           Conv2d-15          [-1, 256, 64, 64]         131,328\n",
            "  ConvTranspose2d-16        [-1, 256, 128, 128]       1,048,832\n",
            "      BatchNorm2d-17        [-1, 256, 128, 128]             512\n",
            "        Waveblock-18        [-1, 256, 128, 128]               0\n",
            "           Conv2d-19         [-1, 64, 128, 128]          16,448\n",
            "           Conv2d-20          [-1, 512, 64, 64]         131,584\n",
            "             GELU-21          [-1, 512, 64, 64]               0\n",
            "          Dropout-22          [-1, 512, 64, 64]               0\n",
            "           Conv2d-23          [-1, 256, 64, 64]         131,328\n",
            "  ConvTranspose2d-24        [-1, 256, 128, 128]       1,048,832\n",
            "      BatchNorm2d-25        [-1, 256, 128, 128]             512\n",
            "        Waveblock-26        [-1, 256, 128, 128]               0\n",
            "           Conv2d-27         [-1, 64, 128, 128]          16,448\n",
            "           Conv2d-28          [-1, 512, 64, 64]         131,584\n",
            "             GELU-29          [-1, 512, 64, 64]               0\n",
            "          Dropout-30          [-1, 512, 64, 64]               0\n",
            "           Conv2d-31          [-1, 256, 64, 64]         131,328\n",
            "  ConvTranspose2d-32        [-1, 256, 128, 128]       1,048,832\n",
            "      BatchNorm2d-33        [-1, 256, 128, 128]             512\n",
            "        Waveblock-34        [-1, 256, 128, 128]               0\n",
            "           Conv2d-35         [-1, 64, 128, 128]          16,448\n",
            "           Conv2d-36          [-1, 512, 64, 64]         131,584\n",
            "             GELU-37          [-1, 512, 64, 64]               0\n",
            "          Dropout-38          [-1, 512, 64, 64]               0\n",
            "           Conv2d-39          [-1, 256, 64, 64]         131,328\n",
            "  ConvTranspose2d-40        [-1, 256, 128, 128]       1,048,832\n",
            "      BatchNorm2d-41        [-1, 256, 128, 128]             512\n",
            "        Waveblock-42        [-1, 256, 128, 128]               0\n",
            "           Conv2d-43         [-1, 64, 128, 128]          16,448\n",
            "           Conv2d-44          [-1, 512, 64, 64]         131,584\n",
            "             GELU-45          [-1, 512, 64, 64]               0\n",
            "          Dropout-46          [-1, 512, 64, 64]               0\n",
            "           Conv2d-47          [-1, 256, 64, 64]         131,328\n",
            "  ConvTranspose2d-48        [-1, 256, 128, 128]       1,048,832\n",
            "      BatchNorm2d-49        [-1, 256, 128, 128]             512\n",
            "        Waveblock-50        [-1, 256, 128, 128]               0\n",
            "           Conv2d-51         [-1, 64, 128, 128]          16,448\n",
            "           Conv2d-52          [-1, 512, 64, 64]         131,584\n",
            "             GELU-53          [-1, 512, 64, 64]               0\n",
            "          Dropout-54          [-1, 512, 64, 64]               0\n",
            "           Conv2d-55          [-1, 256, 64, 64]         131,328\n",
            "  ConvTranspose2d-56        [-1, 256, 128, 128]       1,048,832\n",
            "      BatchNorm2d-57        [-1, 256, 128, 128]             512\n",
            "        Waveblock-58        [-1, 256, 128, 128]               0\n",
            "AdaptiveAvgPool2d-59            [-1, 256, 1, 1]               0\n",
            "        Rearrange-60                  [-1, 256]               0\n",
            "           Linear-61                  [-1, 365]          93,805\n",
            "================================================================\n",
            "Total params: 9,693,485\n",
            "Trainable params: 9,693,485\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.19\n",
            "Forward/backward pass size (MB): 1168.01\n",
            "Params size (MB): 36.98\n",
            "Estimated Total Size (MB): 1205.17\n",
            "----------------------------------------------------------------\n",
            "None\n",
            "_CudaDeviceProperties(name='Tesla V100-SXM2-16GB', major=7, minor=0, total_memory=16160MB, multi_processor_count=80)\n"
          ]
        }
      ],
      "source": [
        "model = WaveMix(\n",
        "    num_classes = 365,\n",
        "    depth = 7,\n",
        "    mult = 2,\n",
        "    ff_channel = 256,\n",
        "    final_dim = 256,\n",
        "    dropout = 0.5\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "print(summary(model, (3, 128, 128)))    \n",
        "print(torch.cuda.get_device_properties(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UUdOglYwPYSz"
      },
      "outputs": [],
      "source": [
        "# model.load_state_dict(torch.load('/content/drive/MyDrive/PhD Thesis/WaveMix256SVHN.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkPiiiZ-PYS0",
        "outputId": "85dc2c04-3eca-42d4-aab2-bd84a4073110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   200] loss: 0.590\n",
            "[1,   400] loss: 0.572\n",
            "[1,   600] loss: 0.560\n",
            "[1,   800] loss: 0.553\n",
            "[1,  1000] loss: 0.544\n",
            "[1,  1200] loss: 0.533\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "top1 = []\n",
        "top5 = []\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
        "for epoch in range(1000):  # loop over the dataset multiple times\n",
        "    t0 = time.time()\n",
        "    epoch_accuracy = 0\n",
        "    epoch_loss = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    model.train()\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            loss = criterion(outputs, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "\n",
        "        acc = (outputs.argmax(dim=1) == labels).float().mean()\n",
        "        epoch_accuracy += acc / len(trainloader)\n",
        "        epoch_loss += loss / len(trainloader)\n",
        "    \n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    correct_1=0\n",
        "    correct_5=0\n",
        "    c = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "#         outputs = net(images)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            res = accuracy(outputs, labels)\n",
        "            correct_1 += res[0][0].float()\n",
        "            correct_5 += res[1][0].float()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            c += 1\n",
        "        \n",
        "    print(f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - Top 1: {correct_1/c:.2f} - Top 5: {correct_5/c:.2f} - Time: {time.time() - t0:.2f}\\n\")\n",
        "#     print(f\"Time: {time.time() - t0:.4f}\\n\")\n",
        "    top1.append(correct_1/c)\n",
        "    top5.append(correct_5/c)\n",
        "    if float(correct_1/c) >= float(max(top1)):\n",
        "        PATH = '/content/drive/MyDrive/PhD Thesis/WaveMix256Places365-128.pth'\n",
        "        torch.save(model.state_dict(), PATH)\n",
        "        print(1)\n",
        "print('Finished Training')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "Places365 WaveMix 256 Lite",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}